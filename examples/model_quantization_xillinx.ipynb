{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitdecentconda195be60078e74141843b635847adeecd",
   "display_name": "Python 3.6.10 64-bit ('decent': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Model quantization\n",
    "\n",
    "To perform model quantization, we use the Xillinx DNNDK tool (https://www.xilinx.com/support/documentation/user_guides/ug1327-dnndk-user-guide.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import tensorflow as tf\n",
    "from scripts import evaluate_graph, freeze_model, prepare_data, artifacts_reporter, train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEST_PATH = 'xillinx_model_compilation_results'\n",
    "DATA_FILE_PATH = os.path.join(os.path.dirname(os.getcwd()), 'datasets/pavia/pavia.npy')\n",
    "GT_FILE_PAT = os.path.join(os.path.dirname(os.getcwd()), 'datasets/pavia/pavia_gt.npy')\n",
    "experiment_dest_path = os.path.join(DEST_PATH, 'experiment_0')\n",
    "data_path = os.path.join(experiment_dest_path, 'data.h5')\n",
    "os.makedirs(experiment_dest_path, exist_ok=True)"
   ]
  },
  {
   "source": [
    "# Prepare the data\n",
    "\n",
    "To fit into the the pipeline, the data has to be preprocessed. It is achieved by the `prepare_data.main` function. It accepts a path to a `.npy` file with the original cube as well as the corresponding ground truth.  In this example, we randomly extract 250 samples from each class (balanced scenario), use 10% of them as validation set, and extract only spectral information of a pixel. The returned object is a dictiornary with three keys: `train`, `test` and `val`. Each of them contains an additional dictionary with `data` and `labels` keys, holding corresponding `numpy.ndarray` objects with the data. For more details about the parameters, refer to the documentation of `prepare_data.main` function (located in `scripts/prepare_data`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data.main(data_file_path=DATA_FILE_PATH,\n",
    "                            ground_truth_path=GT_FILE_PAT,\n",
    "                            output_path=data_path,\n",
    "                            train_size=250,\n",
    "                            val_size=0.1,\n",
    "                            stratified=True,\n",
    "                            background_label=0,\n",
    "                            channels_idx=2,\n",
    "                            neighborhood_size=None,\n",
    "                            save_data=True,\n",
    "                            seed=0)"
   ]
  },
  {
   "source": [
    "# Train the model\n",
    "\n",
    "The function `trian_model.train` executed the trainig procedure. Trained model will be stored under `experiment_dest_path` folder path."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 99, 1, 200)        1200      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 1, 200)         200200    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 828,889\n",
      "Trainable params: 828,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2025 samples, validate on 225 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 1.8422 - acc: 0.2879 - val_loss: 1.3083 - val_acc: 0.5067\n",
      "Epoch 2/200\n",
      " - 1s - loss: 1.0309 - acc: 0.5743 - val_loss: 0.8994 - val_acc: 0.6578\n",
      "Epoch 3/200\n",
      " - 1s - loss: 0.7936 - acc: 0.6741 - val_loss: 0.7704 - val_acc: 0.6533\n",
      "Epoch 4/200\n",
      " - 1s - loss: 0.6843 - acc: 0.7007 - val_loss: 0.6794 - val_acc: 0.7111\n",
      "Epoch 5/200\n",
      " - 1s - loss: 0.6089 - acc: 0.7294 - val_loss: 0.5780 - val_acc: 0.7556\n",
      "Epoch 6/200\n",
      " - 1s - loss: 0.5634 - acc: 0.7551 - val_loss: 0.6491 - val_acc: 0.6622\n",
      "Epoch 7/200\n",
      " - 1s - loss: 0.6292 - acc: 0.7136 - val_loss: 0.5839 - val_acc: 0.7333\n",
      "Epoch 8/200\n",
      " - 1s - loss: 0.5785 - acc: 0.7348 - val_loss: 0.6040 - val_acc: 0.7333\n",
      "Epoch 9/200\n",
      " - 1s - loss: 0.5202 - acc: 0.7728 - val_loss: 0.5533 - val_acc: 0.7644\n",
      "Epoch 10/200\n",
      " - 1s - loss: 0.5129 - acc: 0.7595 - val_loss: 0.6398 - val_acc: 0.6889\n",
      "Epoch 11/200\n",
      " - 1s - loss: 0.5369 - acc: 0.7472 - val_loss: 0.5173 - val_acc: 0.7867\n",
      "Epoch 12/200\n",
      " - 1s - loss: 0.4693 - acc: 0.7960 - val_loss: 0.5385 - val_acc: 0.7733\n",
      "Epoch 13/200\n",
      " - 1s - loss: 0.4647 - acc: 0.7877 - val_loss: 0.4784 - val_acc: 0.7822\n",
      "Epoch 14/200\n",
      " - 1s - loss: 0.4506 - acc: 0.8119 - val_loss: 0.4809 - val_acc: 0.8044\n",
      "Epoch 15/200\n",
      " - 1s - loss: 0.4427 - acc: 0.8094 - val_loss: 0.5339 - val_acc: 0.7600\n",
      "Epoch 16/200\n",
      " - 1s - loss: 0.4732 - acc: 0.8000 - val_loss: 0.5024 - val_acc: 0.7822\n",
      "Epoch 17/200\n",
      " - 1s - loss: 0.4303 - acc: 0.8212 - val_loss: 0.4439 - val_acc: 0.7956\n",
      "Epoch 18/200\n",
      " - 1s - loss: 0.4126 - acc: 0.8232 - val_loss: 0.5071 - val_acc: 0.7511\n",
      "Epoch 19/200\n",
      " - 1s - loss: 0.4481 - acc: 0.8079 - val_loss: 0.4429 - val_acc: 0.7867\n",
      "Epoch 20/200\n",
      " - 1s - loss: 0.3988 - acc: 0.8193 - val_loss: 0.4589 - val_acc: 0.8000\n",
      "Epoch 21/200\n",
      " - 1s - loss: 0.4042 - acc: 0.8311 - val_loss: 0.4262 - val_acc: 0.8311\n",
      "Epoch 22/200\n",
      " - 1s - loss: 0.3924 - acc: 0.8296 - val_loss: 0.4621 - val_acc: 0.7956\n",
      "Epoch 23/200\n",
      " - 1s - loss: 0.3913 - acc: 0.8375 - val_loss: 0.3964 - val_acc: 0.8311\n",
      "Epoch 24/200\n",
      " - 1s - loss: 0.3732 - acc: 0.8326 - val_loss: 0.4519 - val_acc: 0.8044\n",
      "Epoch 25/200\n",
      " - 1s - loss: 0.3657 - acc: 0.8504 - val_loss: 0.4085 - val_acc: 0.8133\n",
      "Epoch 26/200\n",
      " - 1s - loss: 0.3600 - acc: 0.8400 - val_loss: 0.4415 - val_acc: 0.7911\n",
      "Epoch 27/200\n",
      " - 1s - loss: 0.3997 - acc: 0.8272 - val_loss: 0.5154 - val_acc: 0.8089\n",
      "Epoch 28/200\n",
      " - 1s - loss: 0.3833 - acc: 0.8247 - val_loss: 0.3792 - val_acc: 0.8533\n",
      "Epoch 29/200\n",
      " - 1s - loss: 0.3254 - acc: 0.8588 - val_loss: 0.4065 - val_acc: 0.8356\n",
      "Epoch 30/200\n",
      " - 1s - loss: 0.3024 - acc: 0.8726 - val_loss: 0.3515 - val_acc: 0.8311\n",
      "Epoch 31/200\n",
      " - 1s - loss: 0.3095 - acc: 0.8711 - val_loss: 0.3988 - val_acc: 0.8267\n",
      "Epoch 32/200\n",
      " - 1s - loss: 0.3212 - acc: 0.8612 - val_loss: 0.4296 - val_acc: 0.8400\n",
      "Epoch 33/200\n",
      " - 1s - loss: 0.3371 - acc: 0.8519 - val_loss: 0.3559 - val_acc: 0.8222\n",
      "Epoch 34/200\n",
      " - 1s - loss: 0.3154 - acc: 0.8677 - val_loss: 0.3466 - val_acc: 0.8533\n",
      "Epoch 35/200\n",
      " - 1s - loss: 0.3111 - acc: 0.8681 - val_loss: 0.3544 - val_acc: 0.8578\n",
      "Epoch 36/200\n",
      " - 1s - loss: 0.3076 - acc: 0.8672 - val_loss: 0.3931 - val_acc: 0.8044\n",
      "Epoch 37/200\n",
      " - 1s - loss: 0.2790 - acc: 0.8825 - val_loss: 0.3126 - val_acc: 0.8578\n",
      "Epoch 38/200\n",
      " - 1s - loss: 0.2527 - acc: 0.8928 - val_loss: 0.3091 - val_acc: 0.8578\n",
      "Epoch 39/200\n",
      " - 1s - loss: 0.2614 - acc: 0.8943 - val_loss: 0.3228 - val_acc: 0.8489\n",
      "Epoch 40/200\n",
      " - 1s - loss: 0.2505 - acc: 0.9007 - val_loss: 0.3257 - val_acc: 0.8578\n",
      "Epoch 41/200\n",
      " - 1s - loss: 0.2755 - acc: 0.8948 - val_loss: 0.3125 - val_acc: 0.8622\n",
      "Epoch 42/200\n",
      " - 1s - loss: 0.2422 - acc: 0.9012 - val_loss: 0.3505 - val_acc: 0.8444\n",
      "Epoch 43/200\n",
      " - 1s - loss: 0.2456 - acc: 0.8968 - val_loss: 0.3790 - val_acc: 0.8400\n",
      "Epoch 44/200\n",
      " - 1s - loss: 0.3139 - acc: 0.8731 - val_loss: 0.3431 - val_acc: 0.8356\n",
      "Epoch 45/200\n",
      " - 1s - loss: 0.2601 - acc: 0.8914 - val_loss: 0.3762 - val_acc: 0.8267\n",
      "Epoch 46/200\n",
      " - 1s - loss: 0.3392 - acc: 0.8696 - val_loss: 0.3409 - val_acc: 0.8622\n",
      "Epoch 47/200\n",
      " - 1s - loss: 0.2989 - acc: 0.8721 - val_loss: 0.3154 - val_acc: 0.8622\n",
      "Epoch 48/200\n",
      " - 1s - loss: 0.2466 - acc: 0.8938 - val_loss: 0.3140 - val_acc: 0.8756\n",
      "Epoch 49/200\n",
      " - 1s - loss: 0.2479 - acc: 0.8919 - val_loss: 0.3575 - val_acc: 0.8622\n",
      "Epoch 50/200\n",
      " - 1s - loss: 0.2399 - acc: 0.9037 - val_loss: 0.3064 - val_acc: 0.8667\n",
      "Epoch 51/200\n",
      " - 1s - loss: 0.2475 - acc: 0.8983 - val_loss: 0.3457 - val_acc: 0.8533\n",
      "Epoch 52/200\n",
      " - 1s - loss: 0.2664 - acc: 0.8904 - val_loss: 0.2964 - val_acc: 0.8756\n",
      "Epoch 53/200\n",
      " - 1s - loss: 0.2336 - acc: 0.8993 - val_loss: 0.3134 - val_acc: 0.8267\n",
      "Epoch 54/200\n",
      " - 1s - loss: 0.2197 - acc: 0.9072 - val_loss: 0.2924 - val_acc: 0.8889\n",
      "Epoch 55/200\n",
      " - 1s - loss: 0.2336 - acc: 0.9096 - val_loss: 0.2857 - val_acc: 0.8533\n",
      "Epoch 56/200\n",
      " - 1s - loss: 0.2564 - acc: 0.8958 - val_loss: 0.3189 - val_acc: 0.8533\n",
      "Epoch 57/200\n",
      " - 1s - loss: 0.2476 - acc: 0.8943 - val_loss: 0.3064 - val_acc: 0.8578\n",
      "Epoch 58/200\n",
      " - 1s - loss: 0.2254 - acc: 0.9017 - val_loss: 0.4009 - val_acc: 0.8489\n",
      "Epoch 59/200\n",
      " - 1s - loss: 0.2810 - acc: 0.8948 - val_loss: 0.4361 - val_acc: 0.8489\n",
      "Epoch 60/200\n",
      " - 1s - loss: 0.3240 - acc: 0.8751 - val_loss: 0.3060 - val_acc: 0.8400\n",
      "Epoch 61/200\n",
      " - 1s - loss: 0.2621 - acc: 0.8953 - val_loss: 0.2639 - val_acc: 0.8756\n",
      "Epoch 62/200\n",
      " - 1s - loss: 0.2164 - acc: 0.9151 - val_loss: 0.2723 - val_acc: 0.8800\n",
      "Epoch 63/200\n",
      " - 1s - loss: 0.2119 - acc: 0.9116 - val_loss: 0.2417 - val_acc: 0.8711\n",
      "Epoch 64/200\n",
      " - 1s - loss: 0.1981 - acc: 0.9215 - val_loss: 0.2612 - val_acc: 0.8933\n",
      "Epoch 65/200\n",
      " - 1s - loss: 0.1961 - acc: 0.9121 - val_loss: 0.2934 - val_acc: 0.8800\n",
      "Epoch 66/200\n",
      " - 1s - loss: 0.2052 - acc: 0.9146 - val_loss: 0.3195 - val_acc: 0.8578\n",
      "Epoch 67/200\n",
      " - 1s - loss: 0.2068 - acc: 0.9141 - val_loss: 0.2694 - val_acc: 0.8756\n",
      "Epoch 68/200\n",
      " - 1s - loss: 0.2072 - acc: 0.9165 - val_loss: 0.2267 - val_acc: 0.8978\n",
      "Epoch 69/200\n",
      " - 1s - loss: 0.2047 - acc: 0.9215 - val_loss: 0.2601 - val_acc: 0.8756\n",
      "Epoch 70/200\n",
      " - 1s - loss: 0.1815 - acc: 0.9225 - val_loss: 0.2650 - val_acc: 0.8800\n",
      "Epoch 71/200\n",
      " - 1s - loss: 0.1977 - acc: 0.9175 - val_loss: 0.3132 - val_acc: 0.8578\n",
      "Epoch 72/200\n",
      " - 1s - loss: 0.2193 - acc: 0.9057 - val_loss: 0.3581 - val_acc: 0.8667\n",
      "Epoch 73/200\n",
      " - 1s - loss: 0.2970 - acc: 0.8869 - val_loss: 0.3846 - val_acc: 0.8444\n",
      "Epoch 74/200\n",
      " - 1s - loss: 0.2515 - acc: 0.8909 - val_loss: 0.2494 - val_acc: 0.8933\n",
      "Epoch 75/200\n",
      " - 1s - loss: 0.1935 - acc: 0.9254 - val_loss: 0.2458 - val_acc: 0.8667\n",
      "Epoch 76/200\n",
      " - 1s - loss: 0.1934 - acc: 0.9170 - val_loss: 0.2763 - val_acc: 0.8756\n",
      "Epoch 77/200\n",
      " - 1s - loss: 0.1903 - acc: 0.9220 - val_loss: 0.2279 - val_acc: 0.8978\n",
      "Epoch 78/200\n",
      " - 1s - loss: 0.1991 - acc: 0.9210 - val_loss: 0.2820 - val_acc: 0.8978\n",
      "Epoch 79/200\n",
      " - 1s - loss: 0.2067 - acc: 0.9160 - val_loss: 0.2568 - val_acc: 0.8711\n",
      "Epoch 80/200\n",
      " - 1s - loss: 0.2171 - acc: 0.9086 - val_loss: 0.2659 - val_acc: 0.8533\n",
      "Epoch 81/200\n",
      " - 1s - loss: 0.1848 - acc: 0.9289 - val_loss: 0.2334 - val_acc: 0.8889\n",
      "Epoch 82/200\n",
      " - 1s - loss: 0.1818 - acc: 0.9235 - val_loss: 0.2644 - val_acc: 0.8978\n",
      "Epoch 83/200\n",
      " - 1s - loss: 0.1728 - acc: 0.9284 - val_loss: 0.2199 - val_acc: 0.9022\n",
      "Epoch 84/200\n",
      " - 1s - loss: 0.2064 - acc: 0.9160 - val_loss: 0.2372 - val_acc: 0.8844\n",
      "Epoch 85/200\n",
      " - 1s - loss: 0.1979 - acc: 0.9205 - val_loss: 0.3099 - val_acc: 0.8622\n",
      "Epoch 86/200\n",
      " - 1s - loss: 0.1760 - acc: 0.9235 - val_loss: 0.2535 - val_acc: 0.9156\n",
      "Epoch 87/200\n",
      " - 1s - loss: 0.1644 - acc: 0.9333 - val_loss: 0.2405 - val_acc: 0.8978\n",
      "Epoch 88/200\n",
      " - 1s - loss: 0.1650 - acc: 0.9343 - val_loss: 0.3086 - val_acc: 0.8889\n",
      "Epoch 89/200\n",
      " - 1s - loss: 0.1730 - acc: 0.9289 - val_loss: 0.2859 - val_acc: 0.8889\n",
      "Epoch 90/200\n",
      " - 1s - loss: 0.1699 - acc: 0.9294 - val_loss: 0.3125 - val_acc: 0.8711\n",
      "Epoch 91/200\n",
      " - 1s - loss: 0.1696 - acc: 0.9284 - val_loss: 0.2522 - val_acc: 0.9022\n",
      "Epoch 92/200\n",
      " - 1s - loss: 0.2038 - acc: 0.9210 - val_loss: 0.3068 - val_acc: 0.8756\n",
      "Epoch 93/200\n",
      " - 1s - loss: 0.2178 - acc: 0.9136 - val_loss: 0.3161 - val_acc: 0.8756\n",
      "Epoch 94/200\n",
      " - 1s - loss: 0.1981 - acc: 0.9220 - val_loss: 0.2472 - val_acc: 0.8933\n",
      "Epoch 95/200\n",
      " - 1s - loss: 0.1770 - acc: 0.9338 - val_loss: 0.2695 - val_acc: 0.8978\n",
      "Epoch 96/200\n",
      " - 1s - loss: 0.1874 - acc: 0.9244 - val_loss: 0.2263 - val_acc: 0.9156\n",
      "Epoch 97/200\n",
      " - 1s - loss: 0.1770 - acc: 0.9269 - val_loss: 0.2490 - val_acc: 0.8844\n",
      "Epoch 98/200\n",
      " - 1s - loss: 0.1603 - acc: 0.9338 - val_loss: 0.2285 - val_acc: 0.9067\n"
     ]
    }
   ],
   "source": [
    "train_model.train(model_name='model_2d',\n",
    "                    kernel_size=5,\n",
    "                    n_kernels=200,\n",
    "                    n_layers=1,\n",
    "                    dest_path=experiment_dest_path,\n",
    "                    data=data_path,\n",
    "                    sample_size=103,\n",
    "                    n_classes=9,\n",
    "                    lr=0.001,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=2,\n",
    "                    shuffle=True,\n",
    "                    patience=15,\n",
    "                    noise=[],\n",
    "                    noise_sets=[])"
   ]
  },
  {
   "source": [
    "# Evaluate full precision model\n",
    "\n",
    "Evaluate performance of the model in full precision to later compare to the quantized one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/michal/anaconda3/envs/decent/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:542: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n  append_fn(tensor_proto, proto_values)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model.evaluate(\n",
    "    model_path=os.path.join(experiment_dest_path, 'model_2d'),\n",
    "    data=data_path,\n",
    "    dest_path=experiment_dest_path,\n",
    "    n_classes=9,\n",
    "    batch_size=1024,\n",
    "    noise=[],\n",
    "    noise_sets=[])\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "source": [
    "# Freeze model\n",
    "\n",
    "Freeze the tensorflow model into the `.pb` format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Froze 61 variables.\n",
      "INFO:tensorflow:Converted 61 variables to const ops.\n",
      "Frozen model saved at xillinx_model_compilation_results/experiment_0\n"
     ]
    }
   ],
   "source": [
    "freeze_model.main(model_path=os.path.join(experiment_dest_path, 'model_2d'), \n",
    "                  output_dir=experiment_dest_path)"
   ]
  },
  {
   "source": [
    "# Quantize the model\n",
    "\n",
    "Perform the quantization by running the `quantize.sh` bash script with appropriate parameters. It executes the `decent_q` command from the Xillinx DNNDK library. The output is the `quantize_eval_model.pb` file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names_file = os.path.join(experiment_dest_path, 'freeze_input_output_node_name.json')\n",
    "frozen_graph_path = os.path.join(experiment_dest_path, 'frozen_graph.pb')\n",
    "cmd = '../scripts/quantize.sh ' + node_names_file + ' ' \\\n",
    "        + frozen_graph_path + ' ' + data_path + ' ' + \\\n",
    "        '?,103,1,1' + ' ' + \\\n",
    "        'ml_intuition.data.input_fn.calibrate_2d_input' + ' ' + \\\n",
    "        '128' + ' ' + experiment_dest_path + \\\n",
    "        ' ' + str(0)\n",
    "f = open(os.path.join(experiment_dest_path, 'call_output.txt'),'w')\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = os.path.dirname(os.getcwd())\n",
    "subprocess.call(cmd, shell=True, env=env, stderr=f)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "# Evaluate graph\n",
    "\n",
    "Evaluate the performance of the graph to check whether there was any loss in performance. Results for the graph are stored in `inference_graph_metrics.csv`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = os.path.join(experiment_dest_path, 'quantize_eval_model.pb')\n",
    "evaluate_graph.main(graph_path=graph_path,\n",
    "                    node_names_path=node_names_file,\n",
    "                    dataset_path=data_path,\n",
    "                    batch_size=1024)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}