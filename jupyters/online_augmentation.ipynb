{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online augmentation\n",
    "\n",
    "Online augmentaion is a novel approach, which aims at improving the accuracy of generalization of the model. Instead of adding new samples to the training set, online augmentation aims at augmenting each sample from the test set during the inference. N augmented samples and an original one are then classified by the model and majority voting is performed to reveal the final label of a sample. This approach allows the model to 'see' more variations of a sample, and increases the probability of assigning it the correct label.\n",
    "\n",
    "More detailed description of online augmentation can be found in our [paper](https://arxiv.org/abs/1903.05580)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from python_research.keras_models import build_1d_model\n",
    "from python_research.dataset_structures import HyperspectralDataset, BalancedSubset\n",
    "from python_research.augmentation.transformations import PCATransformation, \\\n",
    "    StdDevNoiseTransformation\n",
    "from python_research.augmentation.online_augmenter import OnlineAugmenter\n",
    "\n",
    "DATA_DIR = os.path.join('..', '..', 'hypernet-data')\n",
    "RESULTS_DIR = os.path.join('..', '..', 'hypernet-data', 'results', 'online_augmentation')\n",
    "DATASET_PATH = os.path.join(DATA_DIR, '')\n",
    "GT_PATH = os.path.join(DATA_DIR, '')\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Extract the training, validation and test sets. Trainig set will be balanced (each class will have equal number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to be extracted from each class as training samples\n",
    "SAMPLES_PER_CLASS = 300 \n",
    "# Percentage of the training set to be extracted as validation set \n",
    "VAL_PART = 0.1\n",
    "\n",
    "# Load dataset\n",
    "test_data = HyperspectralDataset(DATASET_PATH, GT_PATH)\n",
    "\n",
    "test_data.normalize_labels()\n",
    "test_data.expand_dims(axis=-1)\n",
    "\n",
    "# Extract training and validation sets\n",
    "train_data = BalancedSubset(test_data, SAMPLES_PER_CLASS)\n",
    "val_data = BalancedSubset(train_data, VAL_PART)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "\n",
    "Data is normalized using Min-Max feature scaling. Min and max values are extracted from train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "max_ = train_data.max if train_data.max > val_data.max else val_data.max\n",
    "min_ = train_data.min if train_data.min < val_data.min else val_data.min\n",
    "train_data.normalize_min_max(min_=min_, max_=max_)\n",
    "val_data.normalize_min_max(min_=min_, max_=max_)\n",
    "test_data.normalize_min_max(min_=min_, max_=max_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of kernels in the first convolutional layer\n",
    "KERNELS = 200 \n",
    "# Size of the kernel in the first convolutional layer\n",
    "KERNEL_SIZE = 5 \n",
    "# Number of classes in the dataset\n",
    "CLASSES_COUNT = 16 \n",
    "# Number of epochs without improvement on validation set after which the \n",
    "# training will be terminated \n",
    "PATIENCE = 15 \n",
    "\n",
    "# Build 1d model\n",
    "model = build_1d_model((test_data.shape[1:]), KERNELS,\n",
    "                       KERNEL_SIZE, CLASSES_COUNT)\n",
    "\n",
    "# Keras Callbacks\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(RESULTS_DIR, \"online_augmentation\") + \"_model\",\n",
    "    save_best_only=True)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "EPOCHS = 200 \n",
    "\n",
    "# Train model\n",
    "history = model.fit(x=train_data.get_data(),\n",
    "                    y=train_data.get_one_hot_labels(CLASSES_COUNT),\n",
    "                    batch_size=64,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early, checkpoint],\n",
    "                    validation_data=(val_data.get_data(),\n",
    "                                     val_data.get_one_hot_labels(CLASSES_COUNT)))\n",
    "# Load best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, \"online_augmentation\") + \"_model\")\n",
    "\n",
    "# Calculate test set score without augmentation\n",
    "test_score = model.evaluate(x=test_data.get_data(),\n",
    "                            y=test_data.get_one_hot_labels(CLASSES_COUNT))\n",
    "print(\"Test set score without online augmentation: {}\".format(test_score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test score evaluation\n",
    "\n",
    "There are four different types of online augmentation implemented:\n",
    "* __Noise injection__ - For band of a given pixel, a random value from normal distribution with mean = 0 and standard deviation equal to the standard deviation of pixel's class and particular band is drawn, multiplied by scaling factor (a = 0.25) and added to the original value\n",
    "* __PCA-based augmentation__ - Method based on PCA. In the first step, principal components are calculated on a training set. Then, a pixel under consideration is transformed using previously calculated principal components, first value of the resulting vector is multiplied by a random value from a given range (0.9 - 1.1 on default), and an inverse transformation is performed on such a vector, resulting in an augmented sample.\n",
    "* __Highlighting/dimming__ - To each band of a given sample, a percentage (10% on default) of an average value of that band (across all samples in the training set) is added (highlighting) or subtracted (dimming)\n",
    "\n",
    "The augmentation is performed using **`OnlineAugmenter`** class, accepting an objects of type **`Transformation`**, which encapsulates the augmentation logic. The **`Transformation`** objects need to call the `fit` method before using it for augmentation, in order to collect all necessary information about the set. Method `evaluate` performs the test set score evaluations, returing overall accuracy and a list with accuracy for each class separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of PCA online augmentation\n",
    "\n",
    "# Remove last dimension (convert column vectors to row vectors)\n",
    "train_data.data = train_data.get_data()[:, :, 0]\n",
    "test_data.data = test_data.get_data()[:, :, 0]\n",
    "# Initialize a transformation and fit the data \n",
    "# (in the case of PCA transformation, it is important \n",
    "# to set the argument `n_components` to be equal to the number of bands \n",
    "# in the dataset, so that the reverse PCA operation does not lose information.\n",
    "pca_transformation = PCATransformation(n_components=train_data.shape[-1])\n",
    "pca_transformation.fit(train_data.get_data())\n",
    "augmenter = OnlineAugmenter()\n",
    "test_score, class_accuracy = augmenter.evaluate(model, test_data,\n",
    "                                                pca_transformation)\n",
    "print(\"Test set score with PCA online augmentation: {}\".format(test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of noise augmentation\n",
    "\n",
    "# Alphas argument indicates by what factors each sample will be multiplied, \n",
    "# indicating how many times each sample will be augmented.\n",
    "noise_transformation = StdDevNoiseTransformation(alphas=[0.1, 0.2])\n",
    "noise_transformation.fit(train_data.get_data())\n",
    "test_score, class_accuracy = augmenter.evaluate(model, test_data,\n",
    "                                                noise_transformation)\n",
    "print(\"Test set score with noise injection online augmentation: {}\".format(test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
