{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch-based segmentation\n",
    "\n",
    "Because Monte Carlo validation method for 3D hyperspectral samples causes data leakage between train and test sets, we introduce a new method of validating 3D segmentation.\n",
    "Instead of randomly picking pixels into sets, we extract _patches_ which are treated as separate images, forming a training set. Patches have fixed size (about 5% of the whole image size) and are drawn randomly until a desired number of pixels is reached (**`TOTAL_NUMBER_OF_TRAIN_SAMPLES`** variable). We have explained this approach in more details in our paper, which can be found [here](https://arxiv.org/abs/1811.03707). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randint\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from utils import load_patches, combine_patches\n",
    "from python_research.dataset_structures import HyperspectralDataset, BalancedSubset\n",
    "from python_research.keras_models import build_3d_model, build_settings_for_dataset, build_1d_model\n",
    "from python_research.grid_extraction import extract_grids\n",
    "%matplotlib inline\n",
    "\n",
    "PATCHES_DIRECTORY = \"\"\n",
    "DATA_DIR = os.path.join('..', '..', 'hypernet-data')\n",
    "RESULTS_DIR = os.path.join('..', '..', 'hypernet-data', 'results', 'grids_validation')\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'PaviaU_corrected.npy')\n",
    "DATASET_GT_PATH = os.path.join(DATA_DIR, 'PaviaU_gt.npy')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "This example allows you to either draw new set of patches to train your model, or use already extracted ones.\n",
    "\n",
    "The **`PATCH_SIZE`** (WIDTH, HEIGHT) variable lets you define the size of extracted patches.<br/>The **`PIXEL_NEIGHBOURHOOD`** defines the spatial size of each sample. If it is equal to 1, 1D samples will be used, using only the spectral\n",
    "information of a pixel. If **`PIXEL_NEIGHBOURHOOD`** variable is different than 1, pixel's neighbourhood of size equal to its value will be extracted for each sample. Neighbourhood of the pixel should be provided as an odd number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = (17, 30)\n",
    "PIXEL_NEIGHBOURHOOD = 1\n",
    "TOTAL_NUMBER_OF_TRAIN_SAMPLES = 2700\n",
    "\n",
    "# Load data if path to the folder with patches is specified\n",
    "if PATCHES_DIRECTORY != \"\":\n",
    "    train_data, test_data = load_patches(PATCHES_DIRECTORY, PIXEL_NEIGHBOURHOOD)\n",
    "    dataset_image = test_data.x[:, :, randint(0, test_data.x.shape[-1])]\n",
    "    train_data.normalize_labels()\n",
    "    test_data.normalize_labels()\n",
    "    bands_count = test_data.shape[-1]\n",
    "    if PIXEL_NEIGHBOURHOOD == 1:\n",
    "        train_data.expand_dims(axis=-1)\n",
    "        test_data.expand_dims(axis=-1)\n",
    "    val_data = BalancedSubset(train_data, 0.1)\n",
    "# Extract grids from provided dataset if path was not specified\n",
    "else:\n",
    "    patches, test_set, dataset_image = extract_grids(DATASET_PATH, DATASET_GT_PATH, PATCH_SIZE, \n",
    "                                                     TOTAL_NUMBER_OF_TRAIN_SAMPLES)\n",
    "    train_data, test_data = combine_patches(patches[0], patches[1], test_set[0], test_set[1], \n",
    "                                            PIXEL_NEIGHBOURHOOD)\n",
    "    train_data.normalize_labels()\n",
    "    test_data.normalize_labels()\n",
    "    if PIXEL_NEIGHBOURHOOD == 1:\n",
    "        train_data.expand_dims(axis=-1)\n",
    "        test_data.expand_dims(axis=-1)\n",
    "    val_data = BalancedSubset(train_data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches visualization\n",
    "\n",
    "Black boxes present pixels (patches) that were extracted into the training and validation sets. In the case of 3D samples, zero-padding was added on the edges of each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data\n",
    "\n",
    "Data is normalized using Min-Max feature scaling. Min and max values are extracted from train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize data\n",
    "max_ = train_data.max if train_data.max > val_data.max else val_data.max\n",
    "min_ = train_data.min if train_data.min < val_data.min else val_data.min\n",
    "train_data.normalize_min_max(min_=min_, max_=max_)\n",
    "val_data.normalize_min_max(min_=min_, max_=max_)\n",
    "test_data.normalize_min_max(min_=min_, max_=max_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "Build the keras model, depending on the dimensionality of samples prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_COUNT = 9\n",
    "NUMBER_OF_FILTERS = 200\n",
    "KERNEL_SIZE = 5\n",
    "PATIENCE = 15\n",
    "\n",
    "if PIXEL_NEIGHBOURHOOD == 1:\n",
    "    model = build_1d_model((test_data.shape[1:]), NUMBER_OF_FILTERS, KERNEL_SIZE, CLASSES_COUNT)\n",
    "else:\n",
    "    settings = build_settings_for_dataset((PIXEL_NEIGHBOURHOOD,\n",
    "                                           PIXEL_NEIGHBOURHOOD))\n",
    "    model = build_3d_model(settings, CLASSES_COUNT, test_data.shape[-1])\n",
    "\n",
    "# Callbacks\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, \"grids_model\"), save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.summary())\n",
    "print(\"Training samples: {}\".format(train_data.shape))\n",
    "print(\"Validation samples: {}\".format(val_data.shape))\n",
    "print(\"Test samples: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(x=train_data.get_data(), \n",
    "          y=train_data.get_one_hot_labels(CLASSES_COUNT), \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=EPOCHS, verbose=False,\n",
    "          callbacks=[early, checkpoint], \n",
    "          validation_data=[val_data.get_data(), val_data.get_one_hot_labels(CLASSES_COUNT)])\n",
    "\n",
    "best_model = load_model(os.path.join(RESULTS_DIR, \"grids_model\"))\n",
    "\n",
    "# Evaluate test set score\n",
    "accuracy = best_model.evaluate(x=test_data.get_data(), y=test_data.get_one_hot_labels(CLASSES_COUNT))[1]\n",
    "print(\"Test set accuracy: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
