{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN)\n",
    "\n",
    "GANs are based on an adversarial process, in which two models are trained simultaneously: a generator network G which learns to synthesize samples from the distribution of provided data, and a discriminator network D which measures the probability that a sample came from the original data, rather than was\n",
    "generated by the generator. During the training stage, both models compete with each other,\n",
    "by G trying to synthesise samples so real, that D can no longer differentiate between real and fake. Implementation presented in this notebook also includes a classifier as a third model. It penalizes the generator for generating samples from wrong class. This addition is particularly useful, because it later allows the user to indicate from which class (distribution) one would like to synthesize the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from python_research.augmentation.GAN.WGAN import WGAN\n",
    "from python_research.augmentation.GAN.classifier import Classifier\n",
    "from python_research.augmentation.GAN.discriminator import Discriminator\n",
    "from python_research.augmentation.GAN.generator import Generator\n",
    "from python_research.keras_models import build_1d_model\n",
    "from python_research.dataset_structures import OrderedDataLoader, \\\n",
    "    HyperspectralDataset, BalancedSubset\n",
    "from python_research.augmentation.GAN.samples_generator import SamplesGenerator\n",
    "\n",
    "DATA_DIR = os.path.join('..', '..', 'hypernet-data')\n",
    "RESULTS_DIR = os.path.join('..', '..', 'hypernet-data', 'results', 'gan_augmentation')\n",
    "DATASET_PATH = os.path.join(DATA_DIR, '')\n",
    "GT_PATH = os.path.join(DATA_DIR, '')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Extract the training, validation and test sets. Trainig set will be balanced (each class will have equal number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to be extracted from each class as training samples\n",
    "SAMPLES_PER_CLASS = 100\n",
    "# Percentage of the training set to be extracted as validation set \n",
    "VAL_PART = 0.1\n",
    "\n",
    "# Load dataset\n",
    "test_data = HyperspectralDataset(DATASET_PATH, GT_PATH)\n",
    "\n",
    "test_data.normalize_labels()\n",
    "\n",
    "# Extract training and validation sets\n",
    "train_data = BalancedSubset(test_data, SAMPLES_PER_CLASS)\n",
    "val_data = BalancedSubset(train_data, VAL_PART)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "\n",
    "Data is normalized using Min-Max feature scaling. Min and max values are extracted from train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "max_ = train_data.max if train_data.max > val_data.max else val_data.max\n",
    "min_ = train_data.min if train_data.min < val_data.min else val_data.min\n",
    "train_data.normalize_min_max(min_=min_, max_=max_)\n",
    "val_data.normalize_min_max(min_=min_, max_=max_)\n",
    "test_data.normalize_min_max(min_=min_, max_=max_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders and models initialization\n",
    "\n",
    "GAN is composed of three models: generator, discriminator and classifier. All of them have an identical topology (2 hidden layers with 512 neurons each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs without improvement on validation set after which the \n",
    "# training will be terminated for the GAN classifier\n",
    "CLASSIFIER_PATIENCE = 30\n",
    "# GAN learning rate\n",
    "LEARNING_RATE = 0.00001\n",
    "# Number of classes in the dataset\n",
    "CLASSES_COUNT = 16 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Initialize pytorch data loaders\n",
    "custom_data_loader = OrderedDataLoader(train_data, BATCH_SIZE)\n",
    "data_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True, drop_last=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "input_shape = bands_count = train_data.shape[-1]\n",
    "\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n",
    "# Initialize generator, discriminator and classifier\n",
    "generator = Generator(input_shape, CLASSES_COUNT)\n",
    "discriminator = Discriminator(input_shape)\n",
    "classifier = Classifier(classifier_criterion, input_shape, CLASSES_COUNT,\n",
    "                        use_cuda=cuda, patience=CLASSIFIER_PATIENCE)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               betas=(0, 0.9))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               betas=(0, 0.9))\n",
    "optimizer_C = torch.optim.Adam(classifier.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               betas=(0, 0.9))\n",
    "# Use GPU if possible\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    classifier_criterion = classifier_criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier pre-training\n",
    "\n",
    "The classifier has to be trained beforehand, so it gives valuable feedback to the generator regarding the classes of samples that it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classifier training epochs\n",
    "CLASSIFIER_EPOCHS = 200\n",
    "\n",
    "# Train classifier\n",
    "classifier.train_(data_loader, optimizer_C, CLASSIFIER_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs without improvement on discriminator loss after\n",
    "# after which the GAN training will be terminated\n",
    "GAN_PATIENCE = 200\n",
    "# Gradient penalty\n",
    "LAMBDA_GP = 10\n",
    "# Number of GAN epochs\n",
    "GAN_EPOCHS = 2000\n",
    "\n",
    "# Initialize GAN\n",
    "gan = WGAN(generator, discriminator, classifier, optimizer_G, optimizer_D,\n",
    "           use_cuda=cuda, lambda_gp=LAMBDA_GP, patience=GAN_PATIENCE)\n",
    "# Train GAN\n",
    "gan.train(custom_data_loader, GAN_EPOCHS, bands_count,\n",
    "          BATCH_SIZE, CLASSES_COUNT, os.path.join(RESULTS_DIR, \"generator_model\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating samples\n",
    "\n",
    "When the training is complete, the generator is used to synthesize new samples. Generation process is performed by the **`SamplesGenerator`** class, using the `generate` method. It accepts training set (in order to calculate number of samples in each class) and the pre-trained generator model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate samples using trained Generator\n",
    "generator = Generator(input_shape, CLASSES_COUNT)\n",
    "generator_path = os.path.join(RESULTS_DIR, \"generator_model\")\n",
    "generator.load_state_dict(torch.load(generator_path))\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "train_data.convert_to_numpy()\n",
    "\n",
    "device = 'gpu' if cuda is True else 'cpu'\n",
    "samples_generator = SamplesGenerator(device=device)\n",
    "generated_x, generated_y = samples_generator.generate(train_data,\n",
    "                                                      generator)\n",
    "# Convert generated Tensors back to numpy\n",
    "generated_x = np.reshape(generated_x.detach().cpu().numpy(),\n",
    "                         generated_x.shape + (1, ))\n",
    "\n",
    "# Add one dimension to convert row vectors to column vectors (keras \n",
    "# requirement) \n",
    "train_data.expand_dims(axis=-1)\n",
    "test_data.expand_dims(axis=-1)\n",
    "val_data.expand_dims(axis=-1)\n",
    "\n",
    "# Add generated samples to original dataset\n",
    "train_data.vstack(generated_x)\n",
    "train_data.hstack(generated_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs without improvement on validation set after which the \n",
    "# training will be terminated \n",
    "PATIENCE = 15 \n",
    "# Number of kernels in the first convolutional layer\n",
    "KERNELS = 200 \n",
    "# Size of the kernel in the first convolutional layer\n",
    "KERNEL_SIZE = 5 \n",
    "# Number of  training epochs\n",
    "EPOCHS = 200\n",
    "\n",
    "# Keras Callbacks\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, \"GAN_augmentation\") + \n",
    "                                          \"_model\",\n",
    "                             save_best_only=True)\n",
    "# Build 1d model\n",
    "model = build_1d_model((test_data.shape[1:]), KERNELS,\n",
    "                       KERNEL_SIZE, CLASSES_COUNT)\n",
    "# Train model\n",
    "history = model.fit(x=train_data.get_data(),\n",
    "                    y=train_data.get_one_hot_labels(CLASSES_COUNT),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early, checkpoint],\n",
    "                    validation_data=(val_data.get_data(),\n",
    "                                     val_data.get_one_hot_labels(CLASSES_COUNT)))\n",
    "# Load best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, \"GAN_augmentation\") + \"_model\")\n",
    "\n",
    "# Calculate test set score with GAN augmentation\n",
    "test_score = model.evaluate(x=test_data.get_data(),\n",
    "                            y=test_data.get_one_hot_labels(CLASSES_COUNT))\n",
    "print(\"Test set score with GAN offline augmentation: {}\".format(test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
