{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline augmentation\n",
    "\n",
    "Offline augmentation is a standard augmentation technique, where the training set is augmented before the training procedure. It has been proved that dataset augmentation improves generalization capabilities of a model.\n",
    "\n",
    "More detailed description of the offline augmentation can be found in our [paper](https://arxiv.org/abs/1903.05580)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from python_research.keras_models import build_1d_model\n",
    "from python_research.dataset_structures import HyperspectralDataset, BalancedSubset\n",
    "from python_research.augmentation.transformations import PCATransformation, \\\n",
    "    StdDevNoiseTransformation\n",
    "from python_research.augmentation.offline_augmenter import OfflineAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', '..', 'hypernet-data')\n",
    "RESULTS_DIR = os.path.join('..', '..', 'hypernet-data', 'results', 'offline_augmentation')\n",
    "DATASET_PATH = os.path.join(DATA_DIR, '')\n",
    "GT_PATH = os.path.join(DATA_DIR, '')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Extract the training, validation and test sets. Trainig set will be balanced (each class will have equal number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to be extracted from each class as training samples\n",
    "SAMPLES_PER_CLASS = 300 \n",
    "# Percentage of the training set to be extracted as validation set \n",
    "VAL_PART = 0.1\n",
    "\n",
    "test_data = HyperspectralDataset(DATASET_PATH, GT_PATH)\n",
    "\n",
    "test_data.normalize_labels()\n",
    "test_data.expand_dims(axis=-1)\n",
    "\n",
    "# Extract training and validation sets\n",
    "train_data = BalancedSubset(test_data, SAMPLES_PER_CLASS)\n",
    "val_data = BalancedSubset(train_data, VAL_PART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "\n",
    "Data is normalized using Min-Max feature scaling. Min and max values are extracted from train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ = train_data.max if train_data.max > val_data.max else val_data.max\n",
    "min_ = train_data.min if train_data.min < val_data.min else val_data.min\n",
    "train_data.normalize_min_max(min_=min_, max_=max_)\n",
    "val_data.normalize_min_max(min_=min_, max_=max_)\n",
    "test_data.normalize_min_max(min_=min_, max_=max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs without improvement on validation set after which the \n",
    "# training will be terminated \n",
    "PATIENCE = 15 \n",
    "# Number of kernels in the first convolutional layer\n",
    "KERNELS = 200 \n",
    "# Size of the kernel in the first convolutional layer\n",
    "KERNEL_SIZE = 5 \n",
    "# Number of classes in the dataset\n",
    "CLASSES_COUNT = 16 \n",
    "\n",
    "# Build 1d model\n",
    "model = build_1d_model((test_data.shape[1:]), KERNELS,\n",
    "                       KERNEL_SIZE, CLASSES_COUNT)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "\n",
    "There are four different types of augmentation implemented:\n",
    "* __Noise injection__ - For band of a given pixel, a random value from normal distribution with mean = 0 and standard deviation equal to the standard deviation of pixel's class and particular band is drawn, multiplied by scaling factor (a = 0.25) and added to the original value\n",
    "* __PCA-based augmentation__ - Method based on PCA. In the first step, principal components are calculated on a training set. Then, a pixel under consideration is transformed using previously calculated principal components, first value of the resulting vector is multiplied by a random value from a given range (0.9 - 1.1 on default), and an inverse transformation is performed on such a vector, resulting in an augmented sample.\n",
    "* __Highlighting/dimming__ - To each band of a given sample, a percentage (10% on default) of an average value of that band (across all samples in the training set) is added (highlighting) or subtracted (dimming)\n",
    "* __Generative Adversarial Network (GAN)__ - covered in a separate jupyter notebok\n",
    "\n",
    "The augmentation is performed using **`OfflineAugmenter`** class, accepting an objects of type **`Transformation`**, which encapsulates the augmentation logic. The **`Transformation`** objects need to call the `fit` method before using it for augmentation, in order to collect all necessary information about the set.\n",
    "\n",
    "The `sampling_mode` argument indicates how many samples will be augmented. If `twice`, the number of samples in each class will be doubled. If `max_twice`, number of samples in each class will be doubled, unless the doubled count exceeds the samples count in the most numerous class, in which case the number of augmented samples will be equal to the difference between those two classes' samples count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last dimension (convert column vectors to row vectors)\n",
    "train_data.data = train_data.get_data()[:, :, 0]\n",
    "\n",
    "# Augment training set using PCA\n",
    "pca_transformation = PCATransformation(n_components=train_data.shape[-1])\n",
    "pca_transformation.fit(train_data.get_data())\n",
    "offline_augmenter = OfflineAugmenter(pca_transformation, sampling_mode='twice')\n",
    "augmented_data, augmented_labels = offline_augmenter.augment(train_data)\n",
    "\n",
    "# Add augmented samples to the training set\n",
    "train_data.vstack(augmented_data)\n",
    "train_data.hstack(augmented_labels)\n",
    "\n",
    "train_data.expand_dims(axis=-1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, \n",
    "                                          \"offline_augmentation_pca_augmented\") + \n",
    "                                          \"_model\",\n",
    "                             save_best_only=True)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 200 \n",
    "# Train model on augmented dataset\n",
    "history = model.fit(x=train_data.get_data(),\n",
    "                    y=train_data.get_one_hot_labels(CLASSES_COUNT),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early, checkpoint],\n",
    "                    validation_data=(val_data.get_data(),\n",
    "                                     val_data.get_one_hot_labels(CLASSES_COUNT)))\n",
    "# Load best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, \n",
    "                                \"offline_augmentation_pca_augmented\") + \n",
    "                                \"_model\")\n",
    "\n",
    "# Calculate test set score with PCA-based augmentation\n",
    "test_score = model.evaluate(x=test_data.get_data(),\n",
    "                            y=test_data.get_one_hot_labels(CLASSES_COUNT))\n",
    "print(\"Test set score with PCA offline augmentation: {}\".format(test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last dimension (convert column vectors to row vectors)\n",
    "train_data.data = train_data.get_data()[:, :, 0]\n",
    "\n",
    "# Augment training set using noise injection\n",
    "noise_transformation = StdDevNoiseTransformation(alphas=[0.1])\n",
    "noise_transformation.fit(train_data.get_data())\n",
    "offline_augmenter = OfflineAugmenter(noise_transformation, \n",
    "                                     sampling_mode='twice')\n",
    "augmented_data, augmented_labels = offline_augmenter.augment(train_data)\n",
    "\n",
    "# Add augmented samples to the training set\n",
    "train_data.vstack(augmented_data)\n",
    "train_data.hstack(augmented_labels)\n",
    "\n",
    "# Convert row vectors to columns vectors (keras requirement)\n",
    "train_data.expand_dims(axis=-1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, \n",
    "                                          \"offline_augmentation_noise_augmented\") + \n",
    "                                          \"_model\",\n",
    "                             save_best_only=True)\n",
    "early = EarlyStopping(patience=PATIENCE)\n",
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 200\n",
    "# Train model on augmented dataset\n",
    "history = model.fit(x=train_data.get_data(),\n",
    "                    y=train_data.get_one_hot_labels(CLASSES_COUNT),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early, checkpoint],\n",
    "                    validation_data=(val_data.get_data(),\n",
    "                                     val_data.get_one_hot_labels(CLASSES_COUNT)))\n",
    "# Load best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, \n",
    "                                \"offline_augmentation_noise_augmented\") + \n",
    "                                \"_model\")\n",
    "\n",
    "# Calculate test set score without augmentation\n",
    "test_score = model.evaluate(x=test_data.get_data(),\n",
    "                            y=test_data.get_one_hot_labels(CLASSES_COUNT))\n",
    "print(\"Test set score with noise injection offline augmentation: {}\".format(test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
